{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Cross Validation in Machine Learning?\n",
    "\n",
    "\n",
    "Cross-validation is a statistical method used to evaluate the performance of a machine learning model. It is a resampling procedure that involves dividing the available data into multiple folds, training the model on a subset of the folds, and then testing the model on the remaining folds. This process is repeated multiple times, with a different fold being used as the test set each time. The results from each of the test sets are then averaged to produce an estimate of the model's performance.\n",
    "\n",
    "Cross-validation is a powerful tool for evaluating machine learning models because it helps to mitigate the effects of overfitting. Overfitting occurs when a model is too closely fit to the training data, and as a result, it does not generalize well to new data. Cross-validation helps to prevent overfitting by training the model on multiple subsets of the data, which helps to ensure that the model is not too closely fit to any one subset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For example,\n",
    "\n",
    "let's say you have a dataset of 100 data points and you want to evaluate the performance of a machine learning model. You could divide the dataset into 10 folds, train the model on 9 of the folds, and then test it on the remaining fold. This would give you an estimate of the model's performance on unseen data.\n",
    "\n",
    "You could repeat this process 10 times, with a different fold being used as the test set each time. The results from each of the test sets would then be averaged to produce an estimate of the model's overall performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Types of cross Validation:\n",
    "\n",
    "K-fold cross-validation: This is the most common cross-validation method. It involves dividing the data into k folds, training the model on k-1 folds, and then testing the model on the remaining fold. This process is repeated k times, with a different fold being used as the test set each time.\n",
    "\n",
    "Leave-one-out cross-validation: This is a special case of k-fold cross-validation where k=n, the number of data points in the dataset. This means that the model is trained on all but one data point, and then tested on the remaining data point. This method is very computationally expensive, but it is also the most accurate method of cross-validation.\n",
    "\n",
    "Repeated k-fold cross-validation: This method involves repeating k-fold cross-validation multiple times. This can help to improve the accuracy of the cross-validation estimate, but it can also be computationally expensive.\n",
    "\n",
    "Stratified k-fold cross-validation: This is a variation of k-fold cross-validation that is used when the data is stratified, meaning that the data points are divided into different groups based on a categorical variable. This ensures that the different groups are represented in the training and test sets.\n",
    "\n",
    "Leave-p-out cross-validation: This is a variation of k-fold cross-validation where p data points are left out of the training set. This is useful when you want to evaluate the model's performance on a specific subset of the data.\n",
    "The best type of cross-validation to use depends on the specific dataset and the machine learning model being used. However, k-fold cross-validation is a good general-purpose cross-validation method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
